{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x161855c4a58>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x= 'label',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you look ,  it is clear case of imbalanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#Removal of punctuations.\n",
    "#Removal of commonly used words (stopwords).\n",
    "#Normalization of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euro2016 people blaming ha for conceded goal was it fat rooney who gave away free kick knowing bale can hit them from there\n",
      "***************************************************\n",
      "***************************************************\n",
      "#euro2016 people blaming ha for conceded goal was it fat rooney who gave away free kick knowing bale can hit them from there.  \n"
     ]
    }
   ],
   "source": [
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)             #it would only return words\n",
    "\n",
    "print(form_sentence(df['tweet'].iloc[20]))         #transformed cleaned sentence\n",
    "\n",
    "\n",
    "print('***************************************************\\n***************************************************')\n",
    "\n",
    "\n",
    "print(df['tweet'].iloc[20])                       #orginal sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ireland', 'consumer', 'price', 'index', 'mom', 'climbed', 'previous', 'may', 'blog', 'silver', 'gold', 'forex']\n",
      "***************************************************\n",
      "***************************************************\n",
      " â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n"
     ]
    }
   ],
   "source": [
    "def no_user_alpha(tweet):\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']          #removing user\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]  \n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]   #removing stop word\n",
    "    return clean_mess\n",
    "print(no_user_alpha(form_sentence(df['tweet'].iloc[10])))\n",
    "\n",
    "\n",
    "print('***************************************************\\n***************************************************')\n",
    "\n",
    "\n",
    "print(df['tweet'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'play', 'with', 'my', 'friends', 'with', 'whom', 'I', 'use', 'to', 'play,', 'when', 'you', 'call', 'me', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "tweet_list = 'I was playing with my friends with whom I used to play, when you called me yesterday'.split()\n",
    "print(normalization(tweet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so i am putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(tweet):\n",
    "    \n",
    "    #Generating the list of words in the tweet (hastags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    \n",
    "    new_tweet = form_sentence(tweet)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    \n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train,tweet_test,label_train,label_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_processing at 0x00000161876646A8>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(tweet_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      6314\n",
      "           1       0.17      0.99      0.28        79\n",
      "\n",
      "    accuracy                           0.94      6393\n",
      "   macro avg       0.58      0.96      0.63      6393\n",
      "weighted avg       0.99      0.94      0.96      6393\n",
      "\n",
      "\n",
      "\n",
      "[[5922  392]\n",
      " [   1   78]]\n",
      "0.9385265133740028\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(tweet_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
